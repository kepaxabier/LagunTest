{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/kepa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading latest local model: en-1.0\n",
      "\tLoading embeddings ... \n",
      "\tLoading tokenization model ...\n",
      "\tLoading lemmatization model ...\n",
      "\tLoading tagger model ...\n",
      "\tLoading parser model ...\n",
      "Model loading complete.\n",
      "\n",
      "synset_ids:\n",
      "[Synset('retreat.n.01'), Synset('retreat.n.02'), Synset('retreat.n.03'), Synset('retreat.n.04'), Synset('hideaway.n.02'), Synset('retirement.n.03'), Synset('retreat.n.07'), Synset('withdraw.v.01'), Synset('retreat.v.02'), Synset('retrograde.v.03'), Synset('retreat.v.04')]\n",
      "1\tretreated({'pull_back', 'draw_back', 'retire', 'move_back', 'withdraw', 'pull_away', 'recede'})\tretreat\tVERB\tVBN\tMood=Ind|Tense=Past|VerbForm=Fin\t0\troot\tSpaceAfter=No\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "from wordfreq import zipf_frequency\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def set_is_empty(some_set):\n",
    "    return some_set == set()\n",
    "#def plot_wordcloud(wordcloud):\n",
    "#    plt.imshow(wordcloud)\n",
    "#    plt.axis(\"off\")\n",
    "#    plt.show()\n",
    "#Este método devuelve true en caso de que la palabra pasada como parámetro sea verbo. Para que una palabra sea \n",
    "#verbo se tiene que cumplir que sea VERB o que sea AUX y que su padre NO sea VERB.\n",
    "\n",
    "####analizador sintactico#######################\n",
    "from cube.api import Cube\n",
    "cube=Cube(verbose=True)\n",
    "#sudo chown -R www-data:www-data /var/www ya que baja el modelo a /var/www/.nlpcube/models/en-1.1\n",
    "cube.load(\"en\") \n",
    "\n",
    "####input file##################\n",
    "input=\"Primerparrafo.txt\"#sys.argv[1]\n",
    "####ouput files#################\n",
    "#estadisticos\n",
    "estadisticaoutput=input+\".out.csv\"\n",
    "#Write all the information in the file\n",
    "estfile = open(estadisticaoutput, \"w\")\n",
    "\n",
    "###############Tratamiento de texto###############################################\n",
    "#quitar todos los retornos \\n si contiene\n",
    "text = open(input).read().replace('\\n', '')\n",
    "#remove text inside parentheses\n",
    "#text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "#separa , . ! ( ) ? ; del texto con espacios, teniendo en cuenta que los no son numeros en el caso de , y . \n",
    "text = re.sub(r'[.]+(?![0-9])', r' . ', text)\n",
    "text = re.sub(r'[,]+(?![0-9])', r' , ', text)\n",
    "text = re.sub(r\"!\", \" ! \", text)\n",
    "text = re.sub(r\"\\(\", \" ( \", text)\n",
    "text = re.sub(r\"\\)\", \" ) \", text)\n",
    "text = re.sub(r\"\\?\", \" ? \", text)\n",
    "text = re.sub(r\";\", \" ; \", text)\n",
    "#sustituye 2 espacios seguidos por 1\n",
    "text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "##############################################################################################\n",
    "#Generating a word cloud with no optional parameters based on the above string:\n",
    "from wordcloud import STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add(\"every\")\n",
    "stopwords.add(\"will\")\n",
    "#stopwords={'to', 'of', 'us'}\n",
    "#Generating a word cloud with no optional parameters based on the above string:\n",
    "# #This is because the wordcloud module ignores stopwords by default. Refer to Part 1 of the NLTK tutorial if the concept of stopwords is new to you.If we wish, we can specify our own set of stopwords, instead of the stopwords provided by default.\n",
    "# #Con relative_scaling = 0, solo se consideran los rangos de las palabras. Si modificamos esto a relative_scaling = 1.0, entonces una palabra que aparece dos veces más frecuentemente aparecerá dos veces el tamaño. Por defecto, relative_scaling = 0.5.\n",
    "# wordcloud = WordCloud(relative_scaling=1.0, stopwords={'to', 'of'}).generate(text)\n",
    "wordcloud = WordCloud(relative_scaling=1.0,stopwords=stopwords).generate(text)\n",
    "\n",
    "#Finally, use matplotlib to render the word cloud:\n",
    "#plot_wordcloud(wordcloud)\n",
    "wordcloud.to_file(\"resume.png\") \n",
    "\n",
    "###################analizador morfosintactico#################################################\n",
    "palabras_raras = []\n",
    "palabras_diferentes = []\n",
    "sequences=cube(text)\n",
    "for sequence in sequences:\n",
    "        #Por cada sentencia\n",
    "\tfor entry in sequence:\n",
    "                #Por cada palabra\n",
    "\t\tif entry.word.isalpha() and (entry.upos == 'ADJ' or entry.upos == 'NOUN' or entry.upos == 'VERB' or entry.upos == 'AUX' or entry.upos == 'NOUN' or entry.upos == 'ADV') and entry.word not in palabras_diferentes:\n",
    "\t\t\t#Si la palabra es un content word\n",
    "\t\t\tpalabras_diferentes.append(entry.word)\n",
    "\t\t\twordfrequency = zipf_frequency(entry.word, 'en')\n",
    "\t\t\t#lemafrequency = zipf_frequency(entry.lemma, 'en')\n",
    "\t\t\tif wordfrequency <= 4:\n",
    "                                #Si es rara\n",
    "\t\t\t\tpalabras_raras.append(entry.word)\n",
    "\t\t\t\tlema=entry.lemma\n",
    "\t\t\t\tsynset_ids = wn.synsets(lema)\n",
    "\t\t\t\tif entry.upos=='NOUN':\n",
    "\t\t\t\t\tpatron='.n.'\n",
    "\t\t\t\tif entry.upos=='VERB':\n",
    "\t\t\t\t\tpatron='.v.'\n",
    "\t\t\t\tif entry.upos=='ADJ':\n",
    "\t\t\t\t\tpatron='.a.'\n",
    "\t\t\t\tif entry.upos=='ADV':\n",
    "\t\t\t\t\tpatron='.r.'\n",
    "\t\t\t\tcontador=1\n",
    "\t\t\t\tprint(\"synset_ids:\")\n",
    "\t\t\t\tprint(str(synset_ids))\n",
    "\t\t\t\tfor synset in synset_ids:\n",
    "\t\t\t\t\t#si tiene sinonimos\n",
    "\t\t\t\t\tif patron in synset.name():\n",
    "                                                #si sinonimo del mismo patron \n",
    "\t\t\t\t\t\tif contador==1:\n",
    "\t\t\t\t\t\t\t#La primera inicializo el conjunto a 0\n",
    "\t\t\t\t\t\t\tsynonyms = []\n",
    "\t\t\t\t\t\t\tcontador=contador+1\n",
    "\t\t\t\t\t\t\tfor l in synset.lemmas():\n",
    "\t\t\t\t\t\t\t\tsynonyms.append(l.name())\n",
    "\t\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\t\tsynonyms.remove(entry.word)\n",
    "\t\t\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\t\t\tpass\n",
    "\t\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\t\tsynonyms.remove(entry.lemma)\n",
    "\t\t\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\t\t\tpass\n",
    "\t\t\t\t\t\t\tif not set_is_empty(set(synonyms)):\n",
    "\t\t\t\t\t\t\t\tentry.word=entry.word+\"(\"+str(set(synonyms))+\")\"\n",
    "\t\t\t\t\t\t\tcontador=contador+1\n",
    "\t\tprint(str(entry.index)+\"\\t\"+entry.word+\"\\t\"+entry.lemma+\"\\t\"+entry.upos+\"\\t\"+entry.xpos+\"\\t\"+entry.attrs+\"\\t\"+str(entry.head)+\"\\t\"+str(entry.label)+\"\\t\"+entry.space_after)\n",
    "\t\testfile.write(\"%s\" % str(entry.index)+\"@\"+entry.word+\"@\"+entry.upos)\n",
    "\t\testfile.write(\"\\n\")\n",
    "estfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
